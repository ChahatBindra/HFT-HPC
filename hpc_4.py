# -*- coding: utf-8 -*-
"""HPC_4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a5YLeU9E4UB0lgWu3IigckFPcv_2opVJ
"""

# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the training set
dataset_train = pd.read_csv('/content/drive/MyDrive/HPC/NSE-TATAGLOBAL.csv')
training_set = dataset_train.iloc[:, 1:2].values

dataset_train.head()

# Feature Scaling
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range = (0, 1))
training_set_scaled = sc.fit_transform(training_set)

# Creating a data structure with 60 timesteps and 1 output
X_train = []
y_train = []
for i in range(60, 2035):
    X_train.append(training_set_scaled[i-60:i, 0])
    y_train.append(training_set_scaled[i, 0])
X_train, y_train = np.array(X_train), np.array(y_train)

# Reshaping
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))

#Part 2 - Building the RNN

# Importing the Keras libraries and packages
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout



# Initialising the RNN
regressor = Sequential()

# Adding the first LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
regressor.add(Dropout(0.2))

# Adding a second LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

# Adding a third LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 50, return_sequences = True))
regressor.add(Dropout(0.2))

# Adding a fourth LSTM layer and some Dropout regularisation
regressor.add(LSTM(units = 50))
regressor.add(Dropout(0.2))

import time
import keras

class TimeHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.times = []

    def on_epoch_begin(self, epoch, logs={}):
        self.epoch_time_start = time.time()

    def on_epoch_end(self, epoch, logs={}):
        self.times.append(time.time() - self.epoch_time_start)

time_callback = TimeHistory()

# Adding the output layer
regressor.add(Dense(units = 1))

# Compiling the RNN
regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')

# Fitting the RNN to the Training set
regressor.fit(X_train, y_train, epochs = 100, batch_size = 32,  callbacks=[time_callback])

# Part 3 - Making the predictions and visualising the results

# Getting the real stock price of 2017
dataset_test = pd.read_csv('/content/drive/MyDrive/HPC/tatatest.csv')
real_stock_price = dataset_test.iloc[:, 1:2].values

# Getting the predicted stock price of 2017
dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)
inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values
inputs = inputs.reshape(-1,1)
inputs = sc.transform(inputs)
X_test = []
for i in range(60, 76):
    X_test.append(inputs[i-60:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
predicted_stock_price = regressor.predict(X_test)
predicted_stock_price = sc.inverse_transform(predicted_stock_price)

predicted_stock_price

plt.plot(real_stock_price, color = 'red', label = 'Real TATA Stock Price')
plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted TAT Stock Price')
plt.title('TATA Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('TATA Stock Price')
plt.legend()
plt.show()

import pickle
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

# regressor.save("Model")

# regressor.save("/content/drive/MyDrive/HPC/Model")
X_train.shape[1]

# Initialising the RNN
model = Sequential()

# Adding the first LSTM layer and some Dropout regularisation
model.add(LSTM(units = 50, return_sequences = True, input_shape = (60, 1)))
model.add(Dropout(0.2))
# Adding a second LSTM layer and some Dropout regularisation
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.2))

# Adding a third LSTM layer and some Dropout regularisation
model.add(LSTM(units = 50, return_sequences = True))
model.add(Dropout(0.2))

# Adding a fourth LSTM layer and some Dropout regularisation
model.add(LSTM(units = 50))
model.add(Dropout(0.2))

from tensorflow import keras
model = keras.models.load_model('/content/drive/MyDrive/HPC/Model')

# Part 3 - Making the predictions and visualising the results

# Getting the real stock price of 2017
dataset_test = pd.read_csv('/content/drive/MyDrive/HPC/tatatest.csv')
real_stock_price = dataset_test.iloc[:, 1:2].values

# Getting the predicted stock price of 2017
dataset_total_2 = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)
inputs = dataset_total_2[len(dataset_total_2) - len(dataset_test) - 60:].values
inputs = inputs.reshape(-1,1)
inputs = sc.transform(inputs)
X_test = []
for i in range(60, 76):
    X_test.append(inputs[i-60:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
predicted_stock_price = model.predict(X_test)

predicted_stock_price = sc.inverse_transform(predicted_stock_price)
predicted_stock_price

plt.plot(real_stock_price, color = 'red', label = 'Real TATA Stock Price')
plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted TAT Stock Price')
plt.title('TATA Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('TATA Stock Price')
plt.legend()
plt.show()

# from google.colab import drive
# drive.mount('/content/drive')

left = []
for i in range(len(time_callback.times)):
  left.append(i+1)

import matplotlib.pyplot as plt

# left = [1, 2, 3, 4, 5]

height = time_callback.times
  
# labels for bars
# tick_label = ['one', 'two', 'three', 'four', 'five']
  
# plotting a bar chart
plt.bar( left[1:],height[1:],width = 0.8, color = ['red', 'green', "blue"])
  
# naming the x-axis
plt.xlabel('Steps')
# naming the y-axis
plt.ylabel('Time Taken')
# plot title
plt.title('My bar chart!')
  
# function to show the plot
plt.show()

print("Average = " , sum(time_callback.times)/len(time_callback.times))

epochs = [50,100,150,200,250,300]
batch_size = [8,16,32,64,128,256]

time_epochs = []

import time
for i in epochs:
  start_time = time.time()
  # Adding the output layer
  regressor.add(Dense(units = 1))

  # Compiling the RNN
  regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')

  # Fitting the RNN to the Training set
  history = regressor.fit(X_train, y_train, epochs = i, batch_size = 32 )
  time_epochs.append(time.time() - start_time)

import matplotlib.pyplot as plt

plt.bar( epochs,time_epochs,width = 20, color = ['red', 'green', "blue"])
  
# naming the x-axis
plt.xlabel('Epochs ')
# naming the y-axis
plt.ylabel('Time Taken')
# plot title
plt.title('Variation in epochs!')
  
# function to show the plot
plt.show()

time_batch = []

import time
for i in batch_size:
  start_time = time.time()
  # Adding the output layer
  regressor.add(Dense(units = 1))

  # Compiling the RNN
  regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')

  # Fitting the RNN to the Training set
  history = regressor.fit(X_train, y_train, epochs = 100, batch_size = i )
  time_batch.append(time.time() - start_time)

import matplotlib.pyplot as plt

plt.bar( [1,2,3,4,5,6],time_batch,width = 0.8, color = ['red', 'green', "blue"])
  
# naming the x-axis
plt.xlabel('Batch Size ')
# naming the y-axis
plt.ylabel('Time Taken')
# plot title
plt.title('Variation in batch size!')
  
# function to show the plot
plt.show()



time_batch

time_epochs

